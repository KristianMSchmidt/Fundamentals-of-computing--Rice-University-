
EFFICIENCY, QUESTION 4:
Which clustering method is faster when the number of output clusters is either a small fixed number or a small fraction of the number of input
 clusters? Provide a short explanation in terms of the asymptotic running times of both methods. You should assume that hierarchical_clustering
 uses fast_closest_pair and that k-means clustering always uses a small fixed number of iterations.

Input size: n number of input clusters

If the number of desired output clusters is small fixed number (for example 1), then the run times are as follows:

Run times:
Hierarchical_clusetring O(n^2 * log^2(n))

kmeans clustering is O(number of input clusters * number of ouput clusters  * number of iterations) = O(n * k ) = O(n)

If the number of desired output clusters is small fraction of n, then the run times are as follows:

Hierarchical_clustering: same big O! (as can be argued mathematically)

kmeans: O(n* n*lille_br√∏kdel * k ) = O(n^2*k= O(n^2)  (but since the fraction is SMALL, then in practice kmeans
will still be a lot faster)



Distortion of clustering - 111 datapoints 9 clusters:
Hierarchical: 1.75163886916e+11
k_means: 2.71254226924e+11 (5 iterations)

Hierarchical clustering methods requires LESS human supervision than k_means to produce results with low distorsion.

from solution:"""Hierarchical clustering requires less human supervision than k-means clustering to produce clustering of relatively
low distortion as it requires no human interaction beyond the choice of the number of output clusters.
 On the other hand, k-means clustering requires a good strategy for choosing the initial cluster centers.
  As we saw in Question 8, strategies that appear reasonable at first glance (i.e. place initial clusters a biggest population
  centers) may lead to clusterings with relatively high distortion."""

from solution:"""In general, k-means clustering requires a higher level of human supervision since the quality of the output clustering is sensitive to the initial choice of cluster centers. k-means clustering also requires that the size of the output clustering be known beforehand. For hierarchical clustering,
 the distortion of the clustering can be monitored as closest pairs of clusters are merged with almost no extra cost.
Question 11 (1 pt)"""


For each data set (111, 290, and 896 counties), does one clustering method consistently produce lower distortion clusterings
when the number of output clusters is in the range 6 to 20? Is so, indicate on which data set(s) one method is superior to the other.
Answer: Surprising, the overall impression is not black and white: Sometimes kmeans produceses less distorsion, sometimes vice versa.
When the datasets are large, kmeans and hierarchical are quite similar, when it comes to distortion. However, it looks
like on the small datasets (111 counties), hierarchical is consistingly better than kmeans.


Conclusion: On the large datasets I would prefer kmeans, at it  ALOT faster, and since it is okay precise.
On small datasets, I would definitely prefer hierarchical clustering.







""Application #3 Solution
Honor Code
To aid you in assessing your peers' submissions for the Application, we have developed an extensive solution/grading guide provided below that you should review after completing your first submission to the Application and before you assess your peers' submissions. Please do not consult this reading before making your first attempt at the Application. Using this reading to prepare your first submission violates the class Honor Code and robs you of the learning experience provided by this assignment. If you need to make a second submission for the Application, we ask that you limit your consultation of this solution guide and under no circumstances should you submit copies of the plots provided below. Create your own original plots.

Solution for Question 1 (2 pts)
Item a (1 pt) Does the plot follow the formatting guidelines for plots? Does the plot include a legend?

The formatting guidelines include the following items:

The plot is an image and not a text file.
The plot is appropriately trimmed. Showing the boundary of the plot's window is fine. However, the plot should not include part of the desktop.
The elements of the plot are of the correct type. Line plots are not the same as point plots.
Both axes should have tick marks labeled by regularly-spaced coordinate values.
Both axes have appropriate text labels that describe the quantities being plotted.
The plot has an appropriate title that describes the content of the plot.
The plot has an appropriate legend (when required) that distinguishes the various components of the plot.
Assess the submitted plot based on these guidelines. For this question, do not deduct if the title of the plot does not distinguish between desktop Python and CodeSkulptor.

Item b (1 pt) Do the two curves in the plot have the correct shapes?

The running times of the functions slow_closest_pair and fast_closest_pair should be O(n2) and O(nlog2n), respectively. Here are correct plots in both matplotlib and simpleplot.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_closest_time.png

Image: http://storage.googleapis.com/codeskulptor-alg/simpleplot_closest_time.png

For fast_closest_pair, the timing curve should look almost linear with a very slight upward bend, indicating the function is growing at an O(nlog2n) rate. If the timing curve appears to be growing quadratically (i.e. it is a constant fraction of the timing curve for slow_closest_pair), count this item as incorrect. If you are unsure, score your peer's answer as correct.

Solution for Question 2 (1 pt)
The submitted image will be assessed based on whether it matches our solution image. You do not need to include axes, axes labels, or a title for this image.

Our code for computing this image takes approximately 10 minutes to run in IDLE so we will only provide the matplotlib solution image below. Observe that the solution image has a small green cluster in the far upper right (to the right of the olive cluster). Images created with other tools such as simplegui or GNUPlot are fine as long as they include the map of the USA in the background and the shape of the clusters match those in the the image below fairly accurately.

Compare the shape of the clusters in the submitted image to the shape of the clusters in the solution image. Since hierarchical clustering is deterministic, the shape of the clusters (where each cluster is visualized as a collection of counties) should match exactly. Do not compare the colors of the clusters. Reordering the clusters (which is allowed) causes the visualization code to assign different colors to each cluster. In some regions, the coloring of adjacent clusters may make it difficult to determine the boundary of the clusters in that region. In these regions, assume that the shape is correct.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_hierarchical_3108_15.png

Images that show the centers of the resulting clusters as well as the original counties are acceptable. Again, the position of each cluster center should exactly match the solution below. The absolute size of each cluster center need not match the solution image below since submitted images may have been rescaled. However, the size of each cluster center in relation to its neighbors should agree.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_hierarchical_3108_15_centers.png

Solution for Question 3 (1 pt)
Item a (1 pt) The submitted image will be assessed based on whether it matches our solution image. You do not need to include axes, axes labels, or a title for this image.

Our code for computing this image takes only a few seconds to compute in CodeSkulptor so we will provide both simplegui and matplotlib solution images below. (Note that including the entire window in the simplegui image is fine.)

Compare the shape of the clusters in the submitted image to the shape of the clusters in the solution image. Since the computation of this clustering is deterministic, the shape of the clusters (where each cluster is visualized as a collection of counties) should agree exactly. For example, the correct image should have 4 clusters that touch the east coast of the USA and 4 clusters that touch the west coast of the USA. Even small differences in the shape of the clusters should result in the submitted image being counted as incorrect.

Important note: Compare only the shape of the clusters, not their colors. Reordering the clusters (which is allowed) causes the visualization code to assign different colors to each cluster.

Image: http://storage.googleapis.com/codeskulptor-alg/simpleplot_kmeans_3108_15.png

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_kmeans_3108_15.png

Images that show the centers of the resulting clusters as well as the original counties are acceptable. Again, the position of each cluster center should exactly match the solution. The absolute size of each cluster center need not match the solution image below since submitted images may have been rescaled. However, the size of each cluster center in relation to its neighbors should agree. Finally, the colors of the cluster centers do not need to match.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_kmeans_3108_15_centers.png

Solution for Question 4 (1 pt)
Item a (1 pt) Which clustering method is faster when the number of output clusters is either a small fixed number or a small fraction of the number of input clusters? Provide a short explanation in terms of the asymptotic running times of both methods. You should assume that hierarchical_clustering uses fast_closest_pair and that k-means clustering always uses a small fixed number of iterations.

As your computations in Questions 2 and 3 should have illustrated, k-means clustering is substantially faster than hierarchical clustering when the number of output clusters is fixed or a small fraction of the number of input clusters. The asymptotic analysis of both methods supports this observation.

If there are n input clusters and k output clusters, hierarchical clustering makes n‚àík calls to fast_closest_pair. If k is fixed or a small fraction of n, each call to fast_closest_pair is O(nlog2n) and, therefore, the running time for hierarchical clustering using fast_closest_pair is O(n2log2n). For k-means, the running time is either O(n) or O(n2) depending on whether the size of output cluster is fixed or varies as a function n. Even in the this second case, k being a small fraction of n will reduce the running time in practice.

Note that the submitted explanation does not need to go into this level of detail. However, it should state that k-means clustering is more efficient and include a short explanation that plausibly (possibly correct) matches the explanation above.

Conclusion for Questions 1-4
One important reason that hierarchical clustering is slower than k-means clustering is that each call to fast_closest_pair throws away most of the information used in computing the closest pair. Smarter implementations of hierarchical clustering can substantially improve their performance by using an implementation of dynamic closest pairs. Dynamic methods build an initial data structure that can be used to accelerate a sequence of insertions and deletions from the set of points.

Using dynamic closest pairs leads to implementations of hierarchical clustering that are more competitive with k-means clustering. This paper provides a nice introduction to approaches of this type.

Solution for Question 5 (1 pt)
Item a (1 pt) The submitted image will be assessed based on whether it matches our solution image. You do not need to include axes, axes labels, or a title for this image.

The matplotlib solution image is provided below. Creating the image in simplegui is also fine. (Note that including the entire window in the simplegui image is fine.)

Compare the shape of the clusters in the submitted image to the shape of the clusters in the solution image. Since the computation of this clustering is deterministic, the shape of the clusters (where each cluster is visualized as a collection of counties) should agree exactly. Even small differences in the shape of the clusters should result in the submitted image being counted as incorrect.

Important note: Compare only the shape of the clusters, not their colors. Reordering the clusters (which is allowed) causes the visualization code to assign different colors to each cluster.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_hierarchical_111_9.png

Images that show the centers of the resulting clusters as well as the original counties are acceptable. Again, the position of each cluster center should exactly match the solution. The absolute size of each cluster center need not match the solution image below since submitted images may have been rescaled. However, the size of each cluster center in relation to its neighbors should agree. Finally, the colors of the cluster centers do not need to match.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_hierarchical_111_9_centers.png

Solution for Question 6 (1 pt)
Item a (1 pt) The matplotlib solution image is provided below. Creating the image in simplegui is also fine. (Note that including the entire window in the simplegui image is fine.)

Compare the shape of the clusters in the submitted image to the shape of the clusters in the solution image. Since the computation of this clustering is deterministic, the shape of the clusters (where each cluster is visualized as a collection of counties) should agree exactly. Even small differences in the shape of the clusters should result in the submitted image being counted as incorrect.

Important note: Compare only the shape of the clusters, not their colors. Reordering the clusters (which is allowed) causes the visualization code to assign different colors to each cluster.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_kmeans_111_9.png

Images that show the centers of the resulting clusters as well as the original counties are acceptable. Again, the position of each cluster center should exactly match the solution. The absolute size of each cluster center need not match the solution image below since submitted images may have been rescaled. However, the size of each cluster center in relation to its neighbors should agree. Finally, the colors of the cluster centers do not need to match.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_kmeans_111_9_centers.png

Solution for Question 7 (1 pt)
Item a (1 pt) Enter the values for the distortions for these two clusterings in the box below. Clearly indicate which values correspond to which clustering.

The distortion for the 9 clusters generated by hierarchical clustering on the 111 county data set is between 1.751√ó1011 and 1.752√ó1011. Count any answer in this range as being correct.

The distortion for the 9 clusters generated by k-means clustering on the 111 county data set is between 2.712√ó1011 and 2.713√ó1011. Count any answer in this range as being correct.

Note that the exponential part of the answer (1011) may also be expressed in Python as the string e+11.

Solution for Question 8 (1 pt)
Item a (1 pt) Describe the difference between the shapes of the clusters produced by these two methods on the west coast of the USA. What caused one method to produce a clustering with a much higher distortion?

Each method generates 3 clusters on the west coast of the USA. Hierarchical clustering generates one cluster in Washington state, one in northern California and one in southern California. K-means clustering generates one cluster that includes Washington state and parts of northern California, one cluster that includes the Los Angeles area, and one cluster that includes San Diego. The k-means clustering has substantially higher distortion due in part to the fact that southern California is split into two clusters while northern California is clustered with Washington state.

This difference in cluster shape is due to the fact that the initial clustering used in k-means clustering includes the 3 counties in southern California with high population and no counties in northern California or Washington state. Due to a poor choice of the initial clustering based on large population counties, k-means clustering produces a clustering with relatively high distortion.

When scoring the answer/explanation for this question, you are welcome to use your judgment about the correctness of the submitted explanation. If the explanation is plausible (possibly correct), score it as correct. If not, check whether the submitted explanation mentions that three of the nine initial clusters in k-means clustering are located in southern California. If so, also score these explanations as being correct. Otherwise, score the explanation as being incorrect.

Solution for Question 9 (1 pt)
Item a (1 pt) Based on your answer to Question 8, which method (hierarchical clustering or k-means clustering) requires less human supervision to produce clustering with relatively low distortion?

Hierarchical clustering requires less human supervision than k-means clustering to produce clustering of relatively low distortion as it requires no human interaction beyond the choice of the number of output clusters. On the other hand, k-means clustering requires a good strategy for choosing the initial cluster centers. As we saw in Question 8, strategies that appear reasonable at first glance may lead to clusterings with relatively high distortion.

Conclusion for Questions 5-9
In general, k-means clustering requires a higher level of human supervision since the quality of the output clustering is sensitive to the initial choice of cluster centers. k-means clustering also requires that the size of the output clustering be known beforehand. For hierarchical clustering, the distortion of the clustering can be monitored as closest pairs of clusters are merged with almost no extra cost.

Solution for Question 10 (4 pts)
Item a (1 pt) Do the plots follow the formatting guidelines for plots? Does the title of each plot indicate which data was used to create the plot? Do the plots include a legend?

Review the plotting guidelines from Question 1a. Be generous on the labeling of axes. However, for these plots, it is critical that the title identifies which plot goes with which data set and the legend identifies the clustering method corresponding to each curve. Therefore, score this item as incorrect if either is not present.

Item b (3 pts) Do the two curves in each plot have the correct shapes?

Below are the three plots for the distortion curves for hierarchical clustering and k-means clustering. When assessing each submitted plot, the shape of both curves should match the answer plots closely. Award one point for each correct plot. If one of the methods is producing curves with correct distortion, but the other is incorrect, award one point.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_distortion_111.png

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_distortion_290.png

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_distortion_896.png

Solution for Question 11 (1 pt)
Item a (1 pt) For each data set (111, 290, and 896 counties), does one clustering method consistently produce lower distortion clusterings when the number of output clusters is in the range 6 to 20? Is so, indicate on which data set(s) one method is superior to the other.

For the 111 county data set, hierarchical clustering consistently produces clusterings with less distortion. For the other two data sets, neither clustering method consistently dominates. Interestingly, k-means clustering produces lower distortion clusterings for the 3108 county data set. Here is a plot of distortion for the clusterings produced by both methods.

Image: http://storage.googleapis.com/codeskulptor-alg/matplotlib_distortion_3018.png

For this item, award full credit if the submitted answer states that hierarchical clustering produces lower distortion clusterings for the 111 county data set. Award no credit if the submitted answer states k-means produces lower distortion (or that the results are mixed) for the 111 county data set. Since the results are mixed for the other two data sets, do not consider the answers for these two data sets in scoring this problem.

Conclusion for Questions 10-11
Beyond the smallest data set, neither method consistently produces lower distortion clusterings.

Overall Evaluation (optional, no credit)
Which clustering method would you prefer when analyzing these data sets? Provide a summary of each method's strengths and weaknesses on these data sets in the three areas considered in this application.

On these data sets, neither method dominates in all three areas: efficiency, automation, and quality. In terms of efficiency, k-means clustering is preferable to hierarchical clustering as long as the desired number of output clusters is known beforehand. However, in terms of automation, k-means clustering suffers from the drawback that a reliable method for determining the initial cluster centers needs to be available. Finally, in terms of quality, neither method produces clusterings with consistently lower distortion on larger data sets.

As you may suspect, this answer is basically impossible to reliable peer assess for content since the answer to this question is subject to much interpretation. Feel free to add comments critiquing your peer's answer if you so desire."""
